{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalar Inversion Problem for UQ\n",
    "This is a simple scalar inversion problem used to showcase the intrinsic UQ in ensemble methods. \n",
    "Three different methods are compared: EnKF, EnKF-MDA, and EnRML. Both EnKF-MDA and EnRML are superior to the base EnKF when it comes to UQ ability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import dafi\n",
    "\n",
    "mpl.rcParams.update({'text.usetex': True, 'text.latex.preamble': ['\\\\usepackage{gensymb}'],})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "The state consists of two scalar values\n",
    "\\begin{gather*}\n",
    "    x = \\begin{bmatrix} x_1\\\\ x_2\\end{bmatrix} .\n",
    "\\end{gather*}\n",
    "The observations are of two quantities derived from the state, given by the observation operator as \n",
    "\\begin{gather*}\n",
    "    y = \\mathsf{H}x = \\begin{bmatrix} x_1\\\\ x_1 + x_2^3 \\end{bmatrix} .\n",
    "\\end{gather*}\n",
    "We beleive the prior value to be close to $x=[0.5, 0.5]^\\top$ and use a multivariate normal distribution to represent our confidence in this estimate. The two states are considered independent each with standard deviation of $0.1$. The prior distribution is thus \n",
    "\\begin{gather*}\n",
    "    x \\sim \\mathcal{N}\\left( \\begin{bmatrix} 0.5\\\\ 0.5\\end{bmatrix}, \\begin{bmatrix} 0.1^2 & 0\\\\ 0& 0.1^2\\end{bmatrix} \\right) .\n",
    "\\end{gather*}\n",
    "\n",
    "A single observation with values $y=[0.8, 2.0]^\\top$ is available. The standard deviation associated with each measurement is $0.05$ and the two measurements are independent. \n",
    "The observation is thus \n",
    "\\begin{gather*}\n",
    "    y \\sim \\mathcal{N}\\left( \\begin{bmatrix} 0.8\\\\ 2.0\\end{bmatrix}, \\begin{bmatrix} 0.05^2 & 0\\\\ 0& 0.05^2\\end{bmatrix} \\right) .\n",
    "\\end{gather*}\n",
    "The goal is then to infer the correct state $x$ from this observation and the prior belief. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inputs\n",
    "First we create a dictionary with all the required inputs to DAFI. This will make calling the DAFI.run function easier. \n",
    "The PhysicsModel class for this problem is implemented in the file *model.py*. We will use $1000$ samples and a stopping criteria based on the discrepancy principle (excpet for the EnKF-MDA which needs no stopping criteria) with a factor of $1.2$ and a maximum of $100$ iterations. The EnRML requires the observations to be perturbed only once. This is achieved by specifying 'perturb_obs_option' as 'time' (this is done by the DAFI but specifying it here avoids a warning). The EnKF-MDA required the additional input of number of steps, which we set to $10$. The EnRML requires the additional input of step length, which is between $0$--$1$, and we set as $0.5$. These inverse method--specific inputs are specified in the *inputs_inverse* dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE ONE\n",
    "inverse_method = 'EnKF-MDA' # in ['EnKF', 'EnKF-MDA', 'EnRML']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if inverse_method == 'EnKF':\n",
    "    inputs_dafi = {\n",
    "        'model_file': 'model.py',\n",
    "        'inverse_method': 'EnKF',\n",
    "        'nsamples': 1000,\n",
    "        'max_iterations': 100,\n",
    "        'convergence_option': 'discrepancy',\n",
    "        'convergence_factor': 1.2,\n",
    "    }\n",
    "    \n",
    "    inputs_inverse = {}\n",
    "    \n",
    "elif inverse_method == 'EnKF-MDA':\n",
    "    inputs_dafi = {\n",
    "        'model_file': 'model.py',\n",
    "        'inverse_method': 'EnKF_MDA',\n",
    "        'nsamples': 1000,\n",
    "    }\n",
    "    \n",
    "    inputs_inverse = {'nsteps': 10,}\n",
    "    \n",
    "elif inverse_method == 'EnRML':\n",
    "    inputs_dafi = {\n",
    "        'model_file': 'model.py',\n",
    "        'inverse_method': 'EnRML',\n",
    "        'nsamples': 1000,\n",
    "        'max_iterations': 100,\n",
    "        'convergence_option': 'discrepancy',\n",
    "        'convergence_factor': 1.2, \n",
    "        'perturb_obs_option': 'time',\n",
    "    }\n",
    "    \n",
    "    inputs_inverse = {'step_length': 0.5,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs to the physics model (specified in *model.py*) are the mean and standard deviations of the two prior states and the two observations. These are specified in the *inputs_model* dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_model = {\n",
    "    'x_init_mean': [0.5, 0.5],\n",
    "    'x_init_std': [0.1, 0.1],\n",
    "    'obs': [0.8, 2.0],\n",
    "    'obs_std': [0.05, 0.05],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DAFI\n",
    "With the three sets of inputs specified, we are now ready to run DAFI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = dafi.run(**inputs_dafi, inputs_inverse=inputs_inverse, inputs_model=inputs_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "### Posterior Estimate and Uncertainty\n",
    "First we plot the resulting posterior ensemble. This posterior can be used to obtain an estimate (mean, MAP) and to quantify the uncertainty in this estimate (covariance). Both the samples and a kernel density estimate are plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posteriordf = pd.DataFrame(posterior.T, columns=[\"$x_1$\", \"$x_2$\"])\n",
    "\n",
    "g = sns.JointGrid(x=\"$x_1$\", y=\"$x_2$\", data=posteriordf, space=0)\n",
    "g = g.plot_joint(sns.scatterplot, color='salmon')\n",
    "g = g.plot_marginals(sns.distplot, kde=False, color=\"salmon\", bins=10, norm_hist=True, hist_kws={'rwidth': 0.95})\n",
    "g = g.plot_marginals(sns.kdeplot, color=\"salmon\", shade=False)\n",
    "g.ax_joint.plot(posteriordf['$x_1$'].mean(), posteriordf['$x_2$'].mean(), 'o', markerfacecolor='w' ,markeredgecolor='k')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.kdeplot(posteriordf['$x_1$'], posteriordf['$x_2$'], ax=ax, cmap='Reds', shade=True, shade_lowest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Prior and Posterior\n",
    "We now compare the prior and posterior distributions in both state space and observation space. First some pre-processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prior Gaussian distribution for plotting\n",
    "prior_mean = inputs_model['x_init_mean']\n",
    "prior_cov = np.diag(inputs_model['x_init_std'])\n",
    "prior = np.random.multivariate_normal(prior_mean, prior_cov, 10000)\n",
    "priordf = pd.DataFrame(prior, columns=[\"$x_1$\", \"$x_2$\"])\n",
    "\n",
    "# Create the observation Gaussian distribution for plotting\n",
    "obs_mean = inputs_model['obs']\n",
    "obs_cov = np.diag(inputs_model['obs_std'])\n",
    "obs = np.random.multivariate_normal(obs_mean, obs_cov, 10000)\n",
    "obsdf = pd.DataFrame(obs, columns=[\"$y_1$\", \"$y_2$\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to map the posterior states to observation space. This could be done during the DAFI run if both *analysis_to_obs=True* and *save_level* was set to anything other *None*, in which case the results would be saved to text files. Here however we will use the physics model directly, since for this scalar case it runs quickly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model \n",
    "\n",
    "# initialize the model\n",
    "pmodel = model.Model(inputs_dafi, inputs_model)\n",
    "\n",
    "# map states to observation space\n",
    "prior_obs = pmodel.state_to_observation(prior.T)\n",
    "posterior_obs = pmodel.state_to_observation(posterior)\n",
    "\n",
    "priordf['$y_1$']=prior_obs[0,:]\n",
    "priordf['$y_2$']=prior_obs[1,:]\n",
    "posteriordf['$y_1$']=posterior_obs[0,:]\n",
    "posteriordf['$y_2$']=posterior_obs[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot. In the plots below the prior distribution is in blue, the posterior in red, and the observations in black. First we plot in the state space where it can be seen that the posterior mean has shifted compared to the prior and the posterior covariance is much smaller. Then we plot in the observation space where we see the posterior has gotten much closer to the observation values. \n",
    "\n",
    "Run again with a different choice of inverse method to compare. EnKF tends to collapse, predicting much smaller and incorrect posterior covariance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# options\n",
    "shade = True\n",
    "bw_prior = np.mean(prior_mean)\n",
    "bw_obs = np.mean(obs_mean)\n",
    "lim_state = (-0.5, 1.5)\n",
    "lim_obs = (-0.5, 3.0)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "sns.kdeplot(priordf['$x_1$'], priordf['$x_2$'], ax=axs[0], cmap='bone_r', shade=shade, shade_lowest=False, bw=bw_prior)\n",
    "sns.kdeplot(posteriordf['$x_1$'], posteriordf['$x_2$'], ax=axs[0], cmap='Reds', shade=shade, shade_lowest=False)\n",
    "axs[0].set_aspect('equal', 'box')\n",
    "axs[0].set(xlim=lim_state, ylim=lim_state)\n",
    "\n",
    "sns.kdeplot(obsdf['$y_1$'], obsdf['$y_2$'], ax=axs[1], cmap='Greys', shade=shade, shade_lowest=False, bw=bw_obs)\n",
    "sns.kdeplot(priordf['$y_1$'], priordf['$y_2$'], ax=axs[1], cmap='bone_r', shade=shade, shade_lowest=False, bw=bw_prior)\n",
    "sns.kdeplot(posteriordf['$y_1$'], posteriordf['$y_2$'], ax=axs[1], cmap='Reds', shade=shade, shade_lowest=False)\n",
    "axs[1].set_aspect('equal', 'box')\n",
    "axs[1].set(xlim=lim_obs, ylim=lim_obs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot only the mean values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map prior mean to observation space\n",
    "prior_mean_obs = pmodel.state_to_observation(np.expand_dims(prior_mean, axis=1))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axs[0].plot(prior_mean[0], prior_mean[1], 'bo')\n",
    "axs[0].plot(posteriordf['$x_1$'].mean(), posteriordf['$x_2$'].mean(), 'ro')\n",
    "axs[0].set_aspect('equal', 'box')\n",
    "axs[0].set(xlim=lim_state, ylim=lim_state, xlabel=\"$x_1$\", ylabel=\"$x_2$\")\n",
    "\n",
    "axs[1].plot(obs_mean[0], obs_mean[1], 'kX', markersize=10)\n",
    "axs[1].plot(prior_mean_obs[0], prior_mean_obs[1], 'bo')\n",
    "axs[1].plot(posteriordf['$y_1$'].mean(), posteriordf['$y_2$'].mean(), 'ro')\n",
    "axs[1].set_aspect('equal', 'box')\n",
    "axs[1].set(xlim=lim_obs, ylim=lim_obs, xlabel=\"$y_1$\", ylabel=\"$y_2$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
